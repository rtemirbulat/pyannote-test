{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "To debug a cell, press Alt+Shift+Enter, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install openai torch torchaudio pyannote.audio sklearn pydub",
   "id": "80ad93f6c55459f7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY_HERE>\"\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import openai\n",
    "import torch\n",
    "import torchaudio\n",
    "import pyannote.audio\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "import wave\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import datetime\n",
    "import subprocess\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ],
   "id": "ae7f71ff8bed862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def safe_transcribe(file_obj, retries=3, base_delay=2):\n",
    "    \"\"\"\n",
    "    Calls openai.Audio.transcribe(\"whisper-1\", file_obj), retrying on APIError.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # You can add response_format=\"verbose_json\" to get segments with timestamps\n",
    "            return openai.Audio.transcribe(\n",
    "                model=\"whisper-1\",\n",
    "                file=file_obj,\n",
    "                response_format=\"verbose_json\"\n",
    "            )\n",
    "        except openai.error.APIError as e:\n",
    "            # If it’s the last attempt, re-raise\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "\n",
    "            wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "            logger.warning(f\"APIError on attempt {attempt+1}, waiting {wait_time:.1f}s, error: {e}\")\n",
    "            time.sleep(wait_time)\n"
   ],
   "id": "7a3873e9d20bfd2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    \"text\": \"...\",\n",
    "    \"segments\": [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"seek\": 0,\n",
    "            \"start\": 0.0,\n",
    "            \"end\": 4.0,\n",
    "            \"text\": \"...\",\n",
    "            ...\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n"
   ],
   "id": "ca7ab2b81e37490b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SpeakerDiarizer:\n",
    "    def __init__(self, num_speakers=2):\n",
    "        self.num_speakers = num_speakers\n",
    "        # We no longer load the local whisper model here:\n",
    "        # import whisper\n",
    "        # self.model = whisper.load_model('large')\n",
    "\n",
    "        # Load your speaker embedding model\n",
    "        self.embedding_model = PretrainedSpeakerEmbedding(\n",
    "            \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "    def segment_embedding(self, segment, path, duration):\n",
    "        \"\"\"\n",
    "        Given a single segment with start/end times, read that portion of audio \n",
    "        and compute a speaker embedding vector.\n",
    "        \"\"\"\n",
    "        start = segment[\"start\"]\n",
    "        end = min(duration, segment[\"end\"])\n",
    "        clip = Segment(start, end)\n",
    "\n",
    "        audio = Audio()\n",
    "        waveform, sample_rate = audio.crop(path, clip)\n",
    "\n",
    "        # pass the cropped waveform into the embedding model\n",
    "        embedding = self.embedding_model(waveform[None])\n",
    "        return embedding\n",
    "\n",
    "    def diarize(self, path):\n",
    "        \"\"\"\n",
    "        1) Convert to WAV if needed\n",
    "        2) Transcribe via Whisper API\n",
    "        3) Extract embeddings for each segment\n",
    "        4) Perform speaker clustering\n",
    "        5) Return a text transcript annotated by speaker\n",
    "        \"\"\"\n",
    "\n",
    "        # If file is not already a WAV, convert it to \"audio.wav\"\n",
    "        if not path.endswith('.wav'):\n",
    "            subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
    "            path = 'audio.wav'\n",
    "\n",
    "        # Step 2: Transcribe via Whisper API, with timestamps\n",
    "        with open(path, \"rb\") as file_obj:\n",
    "            result = safe_transcribe(file_obj)\n",
    "\n",
    "        # The Whisper API returns top-level \"text\" plus \"segments\" if we used \"verbose_json\"\n",
    "        segments = result[\"segments\"]\n",
    "\n",
    "        # Get total audio duration\n",
    "        with contextlib.closing(wave.open(path, 'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            duration = frames / float(rate)\n",
    "\n",
    "        # Step 3: Extract embeddings for each segment\n",
    "        embeddings = np.zeros(shape=(len(segments), 192))  # ECAPA embedding = 192-d\n",
    "        for i, segment in enumerate(segments):\n",
    "            embeddings[i] = self.segment_embedding(segment, path, duration)\n",
    "\n",
    "        # Remove any NaNs from the embeddings\n",
    "        embeddings = np.nan_to_num(embeddings)\n",
    "\n",
    "        # Step 4: Cluster the segments into `num_speakers` speakers\n",
    "        clustering = AgglomerativeClustering(self.num_speakers).fit(embeddings)\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        # Attach speaker labels to each segment\n",
    "        for i in range(len(segments)):\n",
    "            segments[i][\"speaker\"] = \"SPEAKER \" + str(labels[i] + 1)\n",
    "\n",
    "        # Step 5: Build a “pretty” transcript with speaker labels and times\n",
    "        def time_fmt(secs):\n",
    "            return str(datetime.timedelta(seconds=round(secs)))\n",
    "\n",
    "        transcript = \"\"\n",
    "        for i, segment in enumerate(segments):\n",
    "            # If this is the first segment or the speaker changed, print a new speaker line\n",
    "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "                transcript += \"\\n\" + segment[\"speaker\"] + \" \" + time_fmt(segment[\"start\"]) + \"\\n\"\n",
    "            # Segment text has a preceding space or punctuation sometimes, so strip as needed\n",
    "            transcript += segment[\"text\"].lstrip() + \" \"\n",
    "\n",
    "        return transcript\n"
   ],
   "id": "bfcfd16347782eb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "diarizer = SpeakerDiarizer(num_speakers=2)\n",
    "transcript = diarizer.diarize(\"your_audio_file.mp3\")  # or .wav, etc.\n",
    "print(transcript)\n"
   ],
   "id": "5eb5e25dd209a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"audio.wav\", \"rb\") as file_obj:\n",
    "    result = openai.Audio.transcribe(\"whisper-1\", file=file_obj, response_format=\"verbose_json\")\n"
   ],
   "id": "9c9177b2a03cad11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
